{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/layal-z/AAI614_Zeineddine/blob/main/week_4/notebook5_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMd66D8BLHkg"
      },
      "source": [
        "# AAI614: Data Science & its Applications\n",
        "\n",
        "*Notebook 5.6: Decision Trees, Random Forests, and Visualizing Model Performance*\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/layal-z/AAI614_Zeineddine/blob/main/week_4/Notebook5_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "*Source: This notebook contains excerpts from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "PWBqJEv3LHki"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY1-tNJ7LHkj"
      },
      "source": [
        "## Part I: Creating a Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUTNheZILHkj"
      },
      "source": [
        "Consider the following two-dimensional data, which has one of four class labels (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "iw5pfiWmLHkj"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4,\n",
        "                  random_state=0, cluster_std=1.0)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deCLEm5ALHkk"
      },
      "source": [
        "A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it.  The following figure presents a visualization of the first four levels of a decision tree classifier for this data.  Notice that after the first split, every point in the upper branch remains unchanged, so there is no need to further subdivide this branch.\n",
        "Except for nodes that contain all of one color, at each level *every* region is again split along one of the two features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GZxI1orLHkk"
      },
      "source": [
        "This process of fitting a decision tree to the data can be done in Scikit-Learn with the ``DecisionTreeClassifier`` estimator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4_8CFXvuLHkk"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier().fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPuDTfOFLHkk"
      },
      "source": [
        "Let's write a utility function to help us visualize the output of the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "dji6WGxYLHkl"
      },
      "outputs": [],
      "source": [
        "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
        "    ax = ax or plt.gca()\n",
        "\n",
        "    # Plot the training points\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
        "               clim=(y.min(), y.max()), zorder=3)\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    # fit the estimator\n",
        "    model.fit(X, y)\n",
        "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
        "                         np.linspace(*ylim, num=200))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    # Create a color plot with the results\n",
        "    n_classes = len(np.unique(y))\n",
        "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
        "                           levels=np.arange(n_classes + 1) - 0.5,\n",
        "                           cmap=cmap, zorder=1)\n",
        "\n",
        "    ax.set(xlim=xlim, ylim=ylim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RftYMvf5LHkl"
      },
      "source": [
        "Now we can examine what the decision tree classification looks like (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "DbCEgBhyLHkl"
      },
      "outputs": [],
      "source": [
        "visualize_classifier(DecisionTreeClassifier(), X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfhe5M9RLHkl"
      },
      "source": [
        "### Decision Trees and Overfitting\n",
        "\n",
        "Such overfitting turns out to be a general property of decision trees: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions it is drawn from.\n",
        "Another way to see this overfitting is to look at models trained on different subsets of the data—for example, in this figure we train two different trees, each on half of the original data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojWH_hnMLHkl"
      },
      "source": [
        "It is clear that in some places the two trees produce consistent results (e.g., in the four corners), while in other places the two trees give very different classifications (e.g., in the regions between any two clusters).\n",
        "The key observation is that the inconsistencies tend to happen where the classification is less certain, and thus by using information from *both* of these trees, we might come up with a better result!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1MsgQ9qLHkm"
      },
      "source": [
        "#  Part II: Working with Random Forests\n",
        "\n",
        "This notion—that multiple overfitting estimators can be combined to reduce the effect of this overfitting—is what underlies an ensemble method called *bagging*.\n",
        "Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which overfits the data, and averages the results to find a better classification.\n",
        "An ensemble of randomized decision trees is known as a *random forest*.\n",
        "\n",
        "This type of bagging classification can be done manually using Scikit-Learn's `BaggingClassifier` meta-estimator, as shown here (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "M4jIGyjkLHkm"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n",
        "                        random_state=1)\n",
        "\n",
        "bag.fit(X, y)\n",
        "visualize_classifier(bag, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_8bo4BhLHkm"
      },
      "source": [
        "In this example, we have randomized the data by fitting each estimator with a random subset of 80% of the training points.\n",
        "In practice, decision trees are more effectively randomized by injecting some stochasticity in how the splits are chosen: this way all the data contributes to the fit each time, but the results of the fit still have the desired randomness.\n",
        "For example, when determining which feature to split on, the randomized tree might select from among the top several features.\n",
        "You can read more technical details about these randomization strategies in the [Scikit-Learn documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest) and references within.\n",
        "\n",
        "In Scikit-Learn, such an optimized ensemble of randomized decision trees is implemented in the `RandomForestClassifier` estimator, which takes care of all the randomization automatically.\n",
        "All you need to do is select a number of estimators, and it will very quickly—in parallel, if desired—fit the ensemble of trees (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "SLHJkCOQLHkm"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "visualize_classifier(model, X, y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwX81TonLHkm"
      },
      "source": [
        "We see that by averaging over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the parameter space should be split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9YqnnuELHkm"
      },
      "source": [
        "## Random Forest Regression\n",
        "\n",
        "In the previous section we considered random forests within the context of classification.\n",
        "Random forests can also be made to work in the case of regression (that is, with continuous rather than categorical variables). The estimator to use for this is the `RandomForestRegressor`, and the syntax is very similar to what we saw earlier.\n",
        "\n",
        "Consider the following data, drawn from the combination of a fast and slow oscillation (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-hmBJWtuLHkm"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(42)\n",
        "x = 10 * rng.rand(200)\n",
        "\n",
        "def model(x, sigma=0.3):\n",
        "    fast_oscillation = np.sin(5 * x)\n",
        "    slow_oscillation = np.sin(0.5 * x)\n",
        "    noise = sigma * rng.randn(len(x))\n",
        "\n",
        "    return slow_oscillation + fast_oscillation + noise\n",
        "\n",
        "y = model(x)\n",
        "plt.errorbar(x, y, 0.3, fmt='o');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxjsK11ALHkn"
      },
      "source": [
        "Using the random forest regressor, we can find the best-fit curve as follows (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "3ZdTR-WeLHkn"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest = RandomForestRegressor(200)\n",
        "forest.fit(x[:, None], y)\n",
        "\n",
        "xfit = np.linspace(0, 10, 1000)\n",
        "yfit = forest.predict(xfit[:, None])\n",
        "ytrue = model(xfit, sigma=0)\n",
        "\n",
        "plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)\n",
        "plt.plot(xfit, yfit, '-r');\n",
        "plt.plot(xfit, ytrue, '-k', alpha=0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7AlaZt3LHkn"
      },
      "source": [
        "Here the true model is shown in the smooth gray curve, while the random forest model is shown by the jagged red curve.\n",
        "The nonparametric random forest model is flexible enough to fit the multiperiod data, without us needing to specifying a multi-period model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp0iKcaMLHkn"
      },
      "source": [
        "## Part III: Random Forest for Classifying Digits\n",
        "\n",
        "In Chapter 38 we worked through an example using the digits dataset included with Scikit-Learn.\n",
        "Let's use that again here to see how the random forest classifier can be applied in this context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "3fq3Ko0vLHkn"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JYA5qtXLHko"
      },
      "source": [
        "To remind us what we're looking at, we'll visualize the first few data points (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "6kgZkryDLHko"
      },
      "outputs": [],
      "source": [
        "# set up the figure\n",
        "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "# plot the digits: each image is 8x8 pixels\n",
        "for i in range(64):\n",
        "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
        "\n",
        "    # label the image with the target value\n",
        "    ax.text(0, 7, str(digits.target[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tz8bZK2LHko"
      },
      "source": [
        "We can classify the digits using a random forest as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "RTjWLebXLHko"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
        "                                                random_state=0)\n",
        "model = RandomForestClassifier(n_estimators=1000)\n",
        "model.fit(Xtrain, ytrain)\n",
        "ypred = model.predict(Xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElOOWek_LHko"
      },
      "source": [
        "Let's look at the classification report for this classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ddr38OUkLHko"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(ypred, ytest))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixuAHsEOLHko"
      },
      "source": [
        "And for good measure, plot the confusion matrix (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "7ZkUdvxNLHko"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "mat = confusion_matrix(ytest, ypred)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
        "            cbar=False, cmap='Blues')\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}